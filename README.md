# ğŸ§  IRIS â€“ Institutional Retrieval-based Information System

**IRIS** is a Hierarchical Retrieval-Augmented Generation (RAG) application designed to answer institution-specific questions using uploaded textual documents. By combining semantic document search with large language models, IRIS delivers **context-aware, accurate, and concise responses** based on relevant institutional knowledge.

---

## ğŸš€ Key Features

- ğŸ” **Hierarchical Retrieval**  
  First filters relevant documents, then narrows down to the most relevant chunks.
  
- ğŸ¤– **LLM-Powered Responses**  
  Uses TogetherAI-hosted models (e.g., Mixtral, LLaMA) for generating fluent, high-quality answers.

- ğŸ“ **Custom Document Support**  
  Simply place `.txt` files in `data/documents/` and theyâ€™re ready for querying.

- ğŸŒ **Web-based UI via Gradio**  
  Easy-to-use interface to ask questions, preview documents, and see retrieval metadata.

- âš™ï¸ **Built with FastAPI, LangChain, ChromaDB, Ollama Embeddings, and Gradio**

---

## ğŸ“‚ Project Structure

iris/
â”œâ”€â”€ gradio_app.py # Gradio frontend
â”œâ”€â”€ main.py # FastAPI backend for retrieval + LLM answer generation
â”œâ”€â”€ rag_hierarchy.py # Core RAG logic: document loading, vector store creation, search
â”œâ”€â”€ vector.py # Standalone script to rebuild ChromaDB
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ data/
â”‚ â””â”€â”€ documents/ # Place your .txt files here
â””â”€â”€ README.md # This file



---

## âš™ï¸ Setup Instructions

### 1. Clone the Repo

git clone https://github.com/yourusername/iris.git
cd iris

2. Set Up Virtual Environment
   python -m venv venv
   source venv/bin/activate   # On Windows: venv\Scripts\activate

3. Install Dependencies
   pip install -r requirements.txt

4. Add Documents
   Put your .txt files inside data/documents/.


ğŸ§  Backend: FastAPI (LLM & Search)

Start the backend API:  python main.py

This starts the IRIS API at http://localhost:5000/ask.

It:
   Performs hierarchical retrieval (document â†’ chunk)
   Generates answers using a TogetherAI-hosted LLM
   Returns source chunks and timing stats



ğŸ’» Frontend: Gradio Interface

Start the frontend:  python gradio_app.py

This launches a local Gradio UI:
                                 Ask questions
                                 View answers and response time
                                 Preview uploaded documents
                                 See which document chunks were retrieved


ğŸ” Environment Variables (Optional)

You can store your Together API key and base URL in a .env file:  TOGETHER_API_KEY=your_api_key_here
                                                                  API_URL=http://localhost:5000/ask




ğŸ“Œ Notes


IRIS uses Ollama Embeddings (mxbai-embed-large, nomic-embed-text) for encoding text and ChromaDB for vector storage.

The retrieval pipeline first selects top documents, then finds best-matching chunks within them.

The LLM is instructed to not quote the documents directly, but synthesize a clean, natural-language answer.

Full markdown formatting supported in answers.







  




